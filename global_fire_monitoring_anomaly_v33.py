#!/usr/bin/env python3
"""
Global Fire Monitoring and Anomaly Reasoning System v3.3
NASA FIRMS + CEDA Fire_cci çµ±åˆç›£è¦–ã‚·ã‚¹ãƒ†ãƒ  + ç•°å¸¸æ¤œçŸ¥ + MiniCPMæ¨è«–

Enhanced Multi-Modal Fire Analysis with Anomaly Detection and AI Reasoning
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import logging
import warnings
warnings.filterwarnings('ignore')

# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ã‚¹è¨­å®š
sys.path.append(str(Path(__file__).parent))
sys.path.append(str(Path(__file__).parent / 'src'))

# v3.2ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ã®ç¶™æ‰¿ç”¨
try:
    # v3.2ã®ã‚³ã‚¢ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆ©ç”¨
    sys.path.append(str(Path(__file__).parent.parent / 'global-fire-monitoring-v3-0'))
    from global_fire_monitoring_v32 import GlobalFireMonitoringSystemV32
    V32_AVAILABLE = True
except ImportError:
    V32_AVAILABLE = False
    print("âš ï¸ v3.2ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

class GlobalFireMonitoringAndAnomalyReasoningSystemV33:
    """
    Global Fire Monitoring and Anomaly Reasoning System v3.3
    
    v3.2ã®æ©Ÿèƒ½ã«åŠ ãˆã¦ä»¥ä¸‹ã‚’è¿½åŠ ï¼š
    - Isolation Forest ã«ã‚ˆã‚‹ç•°å¸¸ã‚°ãƒªãƒƒãƒ‰æ¤œçŸ¥
    - MiniCPM ã«ã‚ˆã‚‹ç•°å¸¸ç†ç”±æ¨è«–
    - ç•°å¸¸ã‚°ãƒªãƒƒãƒ‰ã®å¯è¦–åŒ–ã¨åˆ†æ
    """
    
    def __init__(self, config_path='config/global_config_v33.json'):
        """
        åˆæœŸåŒ–
        
        Args:
            config_path (str): è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        self.config_path = config_path
        self.logger = self._setup_logging()
        self.config = self._load_config()
        
        # v3.2ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        if V32_AVAILABLE:
            try:
                self.v32_system = GlobalFireMonitoringSystemV32()
                self.logger.info("âœ… v3.2ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–æˆåŠŸ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ v3.2ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
                self.v32_system = None
        else:
            self.v32_system = None
            
        # ç•°å¸¸æ¤œçŸ¥ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.anomaly_detector = None
        self.scaler = StandardScaler()
        
        # MiniCPMæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼‰
        self.reasoning_engine = None
        
        # çµæœä¿å­˜ç”¨
        self.latest_results = {}
        
        self.logger.info("ğŸ”¥ Global Fire Monitoring and Anomaly Reasoning System v3.3 åˆæœŸåŒ–å®Œäº†")
    
    def _setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f"fire_monitoring_anomaly_v33_{timestamp}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)
    
    def _load_config(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            if os.path.exists(self.config_path):
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                self.logger.info(f"âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {self.config_path}")
                return config
            else:
                self.logger.warning(f"âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.config_path}")
                return self._create_default_config()
        except Exception as e:
            self.logger.error(f"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_default_config()
    
    def _create_default_config(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®ä½œæˆ"""
        default_config = {
            "system": {
                "name": "Global Fire Monitoring and Anomaly Reasoning System",
                "version": "3.3",
                "description": "Enhanced fire monitoring with anomaly detection and AI reasoning"
            },
            "anomaly_detection": {
                "algorithm": "isolation_forest",
                "contamination": 0.1,
                "n_estimators": 100,
                "random_state": 42,
                "bootstrap": False
            },
            "reasoning": {
                "model": "minicpm",
                "max_tokens": 512,
                "temperature": 0.7
            },
            "data_processing": {
                "min_samples_per_continent": 2000,
                "total_samples": 12500,
                "feature_dimensions": 24
            },
            "visualization": {
                "anomaly_threshold": 0.1,
                "color_scheme": "viridis",
                "figure_size": [15, 10]
            }
        }
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        config_dir = Path(self.config_path).parent
        config_dir.mkdir(exist_ok=True)
        
        with open(self.config_path, 'w', encoding='utf-8') as f:
            json.dump(default_config, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"âœ… ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {self.config_path}")
        return default_config
    
    def process_yearly_data_with_anomaly_detection(self, year=2022):
        """
        å¹´æ¬¡ãƒ‡ãƒ¼ã‚¿å‡¦ç† + ç•°å¸¸æ¤œçŸ¥
        
        Args:
            year (int): å‡¦ç†å¹´
            
        Returns:
            dict: å‡¦ç†çµæœï¼ˆé€šå¸¸ãƒ‡ãƒ¼ã‚¿ + ç•°å¸¸æ¤œçŸ¥çµæœï¼‰
        """
        self.logger.info(f"ğŸ” {year}å¹´ãƒ‡ãƒ¼ã‚¿ã®ç•°å¸¸æ¤œçŸ¥å‡¦ç†é–‹å§‹")
        
        # v3.2ã‚·ã‚¹ãƒ†ãƒ ã§ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å‡¦ç†
        if self.v32_system:
            try:
                base_results = self.v32_system.process_yearly_data(year=year)
                self.logger.info("âœ… v3.2ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†")
            except Exception as e:
                self.logger.error(f"âŒ v3.2ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿
                base_results = self._generate_simulation_data()
        else:
            # v3.2ãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            base_results = self._generate_simulation_data()
        
        # ç•°å¸¸æ¤œçŸ¥ã®å®Ÿè¡Œ
        anomaly_results = self._detect_anomalies(base_results)
        
        # çµæœã®çµ±åˆ
        combined_results = {
            'base_data': base_results,
            'anomaly_detection': anomaly_results,
            'processing_timestamp': datetime.now().isoformat(),
            'year': year
        }
        
        self.latest_results = combined_results
        return combined_results
    
    def _generate_simulation_data(self):
        """ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆv3.2ãŒåˆ©ç”¨ã§ããªã„å ´åˆï¼‰"""
        self.logger.info("ğŸ”„ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
        
        # 12,500ã‚°ãƒªãƒƒãƒ‰ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿
        n_samples = 12500
        
        # å¤§é™¸ã”ã¨ã®ã‚µãƒ³ãƒ—ãƒ«æ•°
        continent_samples = {
            'North America': 2100,
            'South America': 2000,
            'Africa': 2200,
            'Europe': 2000,
            'Asia': 2200,
            'Oceania': 2000
        }
        
        data_list = []
        
        for continent, n_cont_samples in continent_samples.items():
            # å¤§é™¸ã”ã¨ã®ç‰¹å¾´ãƒ‘ã‚¿ãƒ¼ãƒ³
            if continent == 'Africa':
                # ã‚¢ãƒ•ãƒªã‚«ï¼šé«˜ã„ç«ç½æ´»å‹•
                base_fire_activity = np.random.gamma(3, 2, n_cont_samples)
                lat_range = (-35, 37)
                lon_range = (-20, 55)
            elif continent == 'North America':
                # åŒ—ç±³ï¼šä¸­ç¨‹åº¦ã€å­£ç¯€å¤‰å‹•å¤§
                base_fire_activity = np.random.beta(2, 5, n_cont_samples) * 10
                lat_range = (25, 75)
                lon_range = (-170, -50)
            elif continent == 'Asia':
                # ã‚¢ã‚¸ã‚¢ï¼šå¤šæ§˜ãªãƒ‘ã‚¿ãƒ¼ãƒ³
                base_fire_activity = np.random.lognormal(1, 1.5, n_cont_samples)
                lat_range = (-10, 70)
                lon_range = (60, 180)
            else:
                # ãã®ä»–ã®å¤§é™¸
                base_fire_activity = np.random.exponential(2, n_cont_samples)
                lat_range = (-50, 70)
                lon_range = (-180, 180)
            
            # åº§æ¨™ç”Ÿæˆ
            lats = np.random.uniform(lat_range[0], lat_range[1], n_cont_samples)
            lons = np.random.uniform(lon_range[0], lon_range[1], n_cont_samples)
            
            # 24æ¬¡å…ƒç‰¹å¾´é‡ç”Ÿæˆ
            for i in range(n_cont_samples):
                features = {
                    'latitude': lats[i],
                    'longitude': lons[i],
                    'continent': continent,
                    'fire_activity': base_fire_activity[i],
                    'burned_area_total': base_fire_activity[i] * np.random.uniform(0.5, 2.0),
                    'temperature_avg': np.random.normal(25, 10),
                    'precipitation_total': np.random.exponential(50),
                    'vegetation_index': np.random.beta(2, 3),
                    'elevation': abs(np.random.normal(500, 800)),
                    'slope': np.random.gamma(2, 2),
                    'distance_to_water': np.random.exponential(20),
                    'population_density': np.random.lognormal(2, 2),
                }
                
                # è¿‘å‚ç‰¹å¾´é‡ï¼ˆé‡è¦åº¦ãŒé«˜ã„ï¼‰
                features.update({
                    'neighbor_max': features['fire_activity'] * np.random.uniform(1.2, 3.0),
                    'neighbor_std': features['fire_activity'] * np.random.uniform(0.2, 0.8),
                    'neighbor_mean': features['fire_activity'] * np.random.uniform(0.8, 1.5),
                    'neighbor_count': np.random.poisson(5),
                })
                
                # æœˆåˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ12ãƒ¶æœˆï¼‰
                seasonal_pattern = np.sin(np.arange(12) * 2 * np.pi / 12) + 1
                for month in range(12):
                    features[f'month_{month+1}_fire'] = (
                        features['fire_activity'] * seasonal_pattern[month] * 
                        np.random.uniform(0.5, 1.5)
                    )
                
                data_list.append(features)
        
        df = pd.DataFrame(data_list)
        
        self.logger.info(f"âœ… ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {len(df)}ã‚°ãƒªãƒƒãƒ‰")
        return {'processed_data': df, 'metadata': {'data_type': 'simulation'}}
    
    def _detect_anomalies(self, base_results):
        """
        Isolation Forestã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥
        
        Args:
            base_results (dict): v3.2ã‹ã‚‰ã®åŸºæœ¬å‡¦ç†çµæœ
            
        Returns:
            dict: ç•°å¸¸æ¤œçŸ¥çµæœ
        """
        self.logger.info("ğŸ” Isolation Forestç•°å¸¸æ¤œçŸ¥é–‹å§‹")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
            if 'processed_data' in base_results:
                df = base_results['processed_data']
            else:
                raise ValueError("å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            # ç‰¹å¾´é‡ã®é¸æŠï¼ˆæ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            feature_data = df[numeric_columns].copy()
            
            # æ¬ æå€¤ã®å‡¦ç†
            feature_data = feature_data.fillna(feature_data.median())
            
            self.logger.info(f"ğŸ“Š ç‰¹å¾´é‡æ•°: {len(feature_data.columns)}")
            self.logger.info(f"ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(feature_data)}")
            
            # ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åŒ–
            scaled_features = self.scaler.fit_transform(feature_data)
            
            # Isolation Forestè¨­å®š
            anomaly_config = self.config['anomaly_detection']
            
            self.anomaly_detector = IsolationForest(
                contamination=anomaly_config['contamination'],
                n_estimators=anomaly_config['n_estimators'],
                random_state=anomaly_config['random_state'],
                bootstrap=anomaly_config['bootstrap'],
                n_jobs=-1
            )
            
            # ç•°å¸¸æ¤œçŸ¥å®Ÿè¡Œ
            predictions = self.anomaly_detector.fit_predict(scaled_features)
            anomaly_scores = self.anomaly_detector.score_samples(scaled_features)
            
            # çµæœã®çµ±åˆ
            df['anomaly_prediction'] = predictions  # -1: ç•°å¸¸, 1: æ­£å¸¸
            df['anomaly_score'] = anomaly_scores    # ã‚¹ã‚³ã‚¢ï¼ˆä½ã„ã»ã©ç•°å¸¸ï¼‰
            df['is_anomaly'] = (predictions == -1)
            
            # ç•°å¸¸ã‚°ãƒªãƒƒãƒ‰ã®çµ±è¨ˆ
            n_anomalies = (predictions == -1).sum()
            anomaly_rate = n_anomalies / len(predictions)
            
            # å¤§é™¸åˆ¥ç•°å¸¸åˆ†å¸ƒ
            continent_anomalies = df.groupby('continent')['is_anomaly'].agg(['sum', 'count', 'mean'])
            
            # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆç•°å¸¸ã‚°ãƒªãƒƒãƒ‰ã®ç‰¹å¾´åˆ†æï¼‰
            anomaly_df = df[df['is_anomaly']]
            normal_df = df[~df['is_anomaly']]
            
            feature_importance = {}
            for col in numeric_columns:
                if col in ['anomaly_prediction', 'anomaly_score', 'is_anomaly']:
                    continue
                
                try:
                    anomaly_mean = anomaly_df[col].mean()
                    normal_mean = normal_df[col].mean()
                    difference = abs(anomaly_mean - normal_mean) / (normal_mean + 1e-8)
                    feature_importance[col] = difference
                except:
                    feature_importance[col] = 0
            
            # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
            sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
            
            # ç•°å¸¸ã‚°ãƒªãƒƒãƒ‰ã®ã¿ã‚’æŠ½å‡º
            anomaly_grids = df[df['is_anomaly']].copy()
            
            anomaly_results = {
                'detection_summary': {
                    'total_grids': len(predictions),
                    'anomalous_grids': n_anomalies,
                    'anomaly_rate': anomaly_rate,
                    'algorithm': 'isolation_forest',
                    'contamination_rate': anomaly_config['contamination']
                },
                'continent_distribution': continent_anomalies.to_dict(),
                'feature_importance': dict(sorted_features[:10]),  # ä¸Šä½10ç‰¹å¾´
                'anomaly_data': df,  # å…¨ãƒ‡ãƒ¼ã‚¿ï¼ˆç•°å¸¸ãƒ•ãƒ©ã‚°ä»˜ãï¼‰
                'anomaly_grids': anomaly_grids,  # ç•°å¸¸ã‚°ãƒªãƒƒãƒ‰ã®ã¿
                'model_parameters': anomaly_config
            }
            
            self.logger.info(f"âœ… ç•°å¸¸æ¤œçŸ¥å®Œäº†: {n_anomalies}å€‹ã®ç•°å¸¸ã‚°ãƒªãƒƒãƒ‰æ¤œå‡º ({anomaly_rate:.1%})")
            return anomaly_results
            
        except Exception as e:
            self.logger.error(f"âŒ ç•°å¸¸æ¤œçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
            return {'error': str(e)}
    
    def analyze_anomalies_with_reasoning(self, top_n=10):
        """
        ç•°å¸¸ã‚°ãƒªãƒƒãƒ‰ã®è©³ç´°åˆ†æã¨MiniCPMæ¨è«–
        
        Args:
            top_n (int): åˆ†æã™ã‚‹ä¸Šä½ç•°å¸¸ã‚°ãƒªãƒƒãƒ‰æ•°
            
        Returns:
            dict: æ¨è«–çµæœ
        """
        if not self.latest_results or 'anomaly_detection' not in self.latest_results:
            self.logger.error("âŒ ç•°å¸¸æ¤œçŸ¥çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {}
        
        self.logger.info(f"ğŸ¤– ä¸Šä½{top_n}ç•°å¸¸ã‚°ãƒªãƒƒãƒ‰ã®æ¨è«–åˆ†æé–‹å§‹")
        
        try:
            anomaly_data = self.latest_results['anomaly_detection']['anomaly_data']
            anomaly_grids = anomaly_data[anomaly_data['is_anomaly']].copy()
            
            # ç•°å¸¸ã‚¹ã‚³ã‚¢ã§ä¸¦ã¹æ›¿ãˆï¼ˆä½ã„ã»ã©ç•°å¸¸ï¼‰
            anomaly_grids = anomaly_grids.sort_values('anomaly_score').head(top_n)
            
            reasoning_results = []
            
            for idx, row in anomaly_grids.iterrows():
                # å„ç•°å¸¸ã‚°ãƒªãƒƒãƒ‰ã®åˆ†æ
                analysis = self._analyze_single_anomaly(row)
                
                # MiniCPMæ¨è«–ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼å®Ÿè£…ï¼‰
                reasoning = self._generate_anomaly_reasoning(row, analysis)
                
                result = {
                    'grid_id': idx,
                    'location': {
                        'latitude': row['latitude'],
                        'longitude': row['longitude'],
                        'continent': row['continent']
                    },
                    'anomaly_score': row['anomaly_score'],
                    'analysis': analysis,
                    'reasoning': reasoning
                }
                
                reasoning_results.append(result)
            
            self.logger.info(f"âœ… {len(reasoning_results)}å€‹ã®ç•°å¸¸ã‚°ãƒªãƒƒãƒ‰æ¨è«–å®Œäº†")
            
            return {
                'reasoning_results': reasoning_results,
                'summary': self._summarize_anomaly_patterns(reasoning_results),
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ç•°å¸¸æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            return {'error': str(e)}
    
    def _analyze_single_anomaly(self, grid_data):
        """å˜ä¸€ç•°å¸¸ã‚°ãƒªãƒƒãƒ‰ã®åˆ†æ"""
        analysis = {
            'key_features': {},
            'deviation_factors': [],
            'risk_level': 'unknown'
        }
        
        # é‡è¦ç‰¹å¾´é‡ã®åˆ†æ
        important_features = ['fire_activity', 'burned_area_total', 'neighbor_max', 'neighbor_std']
        
        for feature in important_features:
            if feature in grid_data:
                value = grid_data[feature]
                analysis['key_features'][feature] = value
                
                # åå·®è¦å› ã®åˆ¤å®š
                if feature == 'fire_activity' and value > 10:
                    analysis['deviation_factors'].append('æ¥µç«¯ã«é«˜ã„ç«ç½æ´»å‹•')
                elif feature == 'neighbor_max' and value > 15:
                    analysis['deviation_factors'].append('è¿‘å‚ã§ç•°å¸¸ãªé«˜ç«ç½å€¤')
                elif feature == 'neighbor_std' and value > 5:
                    analysis['deviation_factors'].append('è¿‘å‚ç«ç½ã®é«˜ã„å¤‰å‹•æ€§')
        
        # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«åˆ¤å®š
        fire_activity = grid_data.get('fire_activity', 0)
        if fire_activity > 15:
            analysis['risk_level'] = 'critical'
        elif fire_activity > 8:
            analysis['risk_level'] = 'high'
        elif fire_activity > 3:
            analysis['risk_level'] = 'moderate'
        else:
            analysis['risk_level'] = 'low'
        
        return analysis
    
    def _generate_anomaly_reasoning(self, grid_data, analysis):
        """
        MiniCPMæ¨è«–ã®ç”Ÿæˆï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼å®Ÿè£…ï¼‰
        
        å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€MiniCPMãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦æ¨è«–ã‚’ç”Ÿæˆ
        """
        # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼æ¨è«–ãƒ­ã‚¸ãƒƒã‚¯
        continent = grid_data.get('continent', 'Unknown')
        fire_activity = grid_data.get('fire_activity', 0)
        risk_level = analysis['risk_level']
        deviation_factors = analysis['deviation_factors']
        
        # åŸºæœ¬çš„ãªæ¨è«–ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        reasoning_templates = {
            'critical': f"{continent}ã®è©²å½“ã‚°ãƒªãƒƒãƒ‰ã¯æ¥µã‚ã¦æ·±åˆ»ãªç«ç½ç•°å¸¸ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚",
            'high': f"{continent}ã«ãŠã„ã¦é«˜ã„ç«ç½ãƒªã‚¹ã‚¯ãŒæ¤œå‡ºã•ã‚ŒãŸã€‚",
            'moderate': f"{continent}ã§ä¸­ç¨‹åº¦ã®ç«ç½ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦³æ¸¬ã•ã‚ŒãŸã€‚",
            'low': f"{continent}ã§è»½å¾®ãªç«ç½ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç•°å¸¸ãŒç¢ºèªã•ã‚ŒãŸã€‚"
        }
        
        base_reasoning = reasoning_templates.get(risk_level, "ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæ¤œå‡ºã•ã‚ŒãŸã€‚")
        
        # åå·®è¦å› ã®è¿½åŠ 
        if deviation_factors:
            factors_text = "ã€".join(deviation_factors)
            detailed_reasoning = f"{base_reasoning} ä¸»ãªè¦å› : {factors_text}ã€‚"
        else:
            detailed_reasoning = f"{base_reasoning} è©³ç´°ãªè¦å› åˆ†æãŒå¿…è¦ã€‚"
        
        # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        if risk_level == 'critical':
            action = "å³åº§ã®ç¾åœ°èª¿æŸ»ã¨ç·Šæ€¥å¯¾å¿œä½“åˆ¶ã®ç¢ºç«‹ã‚’æ¨å¥¨ã€‚"
        elif risk_level == 'high':
            action = "æ—©æœŸã®ç¾åœ°ç¢ºèªã¨ç›£è¦–å¼·åŒ–ã‚’æ¨å¥¨ã€‚"
        else:
            action = "ç¶™ç¶šçš„ãªç›£è¦–ã¨å®šæœŸçš„ãªãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—ã‚’æ¨å¥¨ã€‚"
        
        full_reasoning = f"{detailed_reasoning} {action}"
        
        return {
            'explanation': full_reasoning,
            'confidence': 0.75,  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ä¿¡é ¼åº¦
            'model': 'minicpm_placeholder',
            'factors_identified': deviation_factors,
            'recommended_action': action
        }
    
    def _summarize_anomaly_patterns(self, reasoning_results):
        """ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¦ç´„"""
        if not reasoning_results:
            return {}
        
        # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ
        risk_levels = [r['analysis']['risk_level'] for r in reasoning_results]
        risk_distribution = {level: risk_levels.count(level) for level in set(risk_levels)}
        
        # å¤§é™¸åˆ†å¸ƒ
        continents = [r['location']['continent'] for r in reasoning_results]
        continent_distribution = {cont: continents.count(cont) for cont in set(continents)}
        
        # å…±é€šè¦å› 
        all_factors = []
        for r in reasoning_results:
            all_factors.extend(r['analysis']['deviation_factors'])
        
        factor_frequency = {factor: all_factors.count(factor) for factor in set(all_factors)}
        
        return {
            'risk_level_distribution': risk_distribution,
            'continent_distribution': continent_distribution,
            'common_factors': dict(sorted(factor_frequency.items(), key=lambda x: x[1], reverse=True)),
            'total_anomalies_analyzed': len(reasoning_results)
        }
    
    def create_anomaly_visualization(self):
        """ç•°å¸¸æ¤œçŸ¥çµæœã®å¯è¦–åŒ–"""
        if not self.latest_results or 'anomaly_detection' not in self.latest_results:
            self.logger.error("âŒ å¯è¦–åŒ–ç”¨ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        self.logger.info("ğŸ“Š ç•°å¸¸æ¤œçŸ¥å¯è¦–åŒ–ä½œæˆé–‹å§‹")
        
        try:
            # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
            plt.rcParams['font.family'] = ['Yu Gothic', 'Hiragino Kaku Gothic Pro', 'Takao Gothic', 'Droid Sans Fallback', 'Arial Unicode MS', 'sans-serif']
            plt.rcParams['axes.unicode_minus'] = False
            
            # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
            anomaly_data = self.latest_results['anomaly_detection']['anomaly_data']
            
            # 6ãƒ‘ãƒãƒ«å¯è¦–åŒ–
            fig, axes = plt.subplots(2, 3, figsize=(18, 12))
            fig.suptitle('Global Fire Monitoring and Anomaly Detection Analysis v3.3', fontsize=16, fontweight='bold')
            
            # 1. ç•°å¸¸ã‚¹ã‚³ã‚¢åˆ†å¸ƒ
            axes[0, 0].hist(anomaly_data['anomaly_score'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')
            axes[0, 0].axvline(anomaly_data['anomaly_score'].quantile(0.1), color='red', linestyle='--', label='Anomaly Threshold')
            axes[0, 0].set_xlabel('Anomaly Score')
            axes[0, 0].set_ylabel('Frequency')
            axes[0, 0].set_title('Anomaly Score Distribution')
            axes[0, 0].legend()
            
            # 2. åœ°ç†çš„åˆ†å¸ƒï¼ˆç•°å¸¸ã‚°ãƒªãƒƒãƒ‰ï¼‰
            normal_data = anomaly_data[~anomaly_data['is_anomaly']]
            anomaly_grids = anomaly_data[anomaly_data['is_anomaly']]
            
            axes[0, 1].scatter(normal_data['longitude'], normal_data['latitude'], 
                             c='lightblue', alpha=0.5, s=10, label='Normal')
            axes[0, 1].scatter(anomaly_grids['longitude'], anomaly_grids['latitude'], 
                             c='red', alpha=0.8, s=30, label='Anomaly')
            axes[0, 1].set_xlabel('Longitude')
            axes[0, 1].set_ylabel('Latitude')
            axes[0, 1].set_title('Geographic Distribution of Anomalies')
            axes[0, 1].legend()
            
            # 3. å¤§é™¸åˆ¥ç•°å¸¸ç‡
            continent_stats = anomaly_data.groupby('continent')['is_anomaly'].agg(['sum', 'count'])
            continent_stats['anomaly_rate'] = continent_stats['sum'] / continent_stats['count']
            
            axes[0, 2].bar(range(len(continent_stats)), continent_stats['anomaly_rate'], 
                          color='orange', alpha=0.7)
            axes[0, 2].set_xticks(range(len(continent_stats)))
            axes[0, 2].set_xticklabels(continent_stats.index, rotation=45, ha='right')
            axes[0, 2].set_ylabel('Anomaly Rate')
            axes[0, 2].set_title('Anomaly Rate by Continent')
            
            # 4. ç«ç½æ´»å‹• vs ç•°å¸¸ã‚¹ã‚³ã‚¢
            axes[1, 0].scatter(anomaly_data['fire_activity'], anomaly_data['anomaly_score'], 
                             c=anomaly_data['is_anomaly'], cmap='RdYlBu', alpha=0.6)
            axes[1, 0].set_xlabel('Fire Activity')
            axes[1, 0].set_ylabel('Anomaly Score')
            axes[1, 0].set_title('Fire Activity vs Anomaly Score')
            
            # 5. ç‰¹å¾´é‡é‡è¦åº¦
            if 'feature_importance' in self.latest_results['anomaly_detection']:
                importance = self.latest_results['anomaly_detection']['feature_importance']
                features = list(importance.keys())[:8]  # ä¸Šä½8ç‰¹å¾´
                values = [importance[f] for f in features]
                
                axes[1, 1].barh(range(len(features)), values, color='green', alpha=0.7)
                axes[1, 1].set_yticks(range(len(features)))
                axes[1, 1].set_yticklabels(features)
                axes[1, 1].set_xlabel('Importance Score')
                axes[1, 1].set_title('Feature Importance for Anomaly Detection')
            
            # 6. ç•°å¸¸ã‚°ãƒªãƒƒãƒ‰è©³ç´°çµ±è¨ˆ
            axes[1, 2].text(0.1, 0.9, f"Total Grids: {len(anomaly_data):,}", transform=axes[1, 2].transAxes, fontsize=12)
            axes[1, 2].text(0.1, 0.8, f"Anomalous Grids: {len(anomaly_grids):,}", transform=axes[1, 2].transAxes, fontsize=12)
            axes[1, 2].text(0.1, 0.7, f"Anomaly Rate: {len(anomaly_grids)/len(anomaly_data):.1%}", transform=axes[1, 2].transAxes, fontsize=12)
            axes[1, 2].text(0.1, 0.6, f"Algorithm: Isolation Forest", transform=axes[1, 2].transAxes, fontsize=12)
            axes[1, 2].text(0.1, 0.5, f"Contamination: {self.config['anomaly_detection']['contamination']}", transform=axes[1, 2].transAxes, fontsize=12)
            axes[1, 2].set_xlim(0, 1)
            axes[1, 2].set_ylim(0, 1)
            axes[1, 2].set_title('Anomaly Detection Summary')
            axes[1, 2].axis('off')
            
            plt.tight_layout()
            
            # ä¿å­˜
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"output/anomaly_detection_analysis_{timestamp}.png"
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            plt.show()
            
            self.logger.info(f"âœ… å¯è¦–åŒ–ä¿å­˜å®Œäº†: {output_path}")
            
        except Exception as e:
            self.logger.error(f"âŒ å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def run_complete_anomaly_analysis(self, year=2022, top_anomalies=10):
        """
        å®Œå…¨ãªç•°å¸¸æ¤œçŸ¥åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        
        Args:
            year (int): åˆ†æå¹´
            top_anomalies (int): è©³ç´°åˆ†æã™ã‚‹ç•°å¸¸ã‚°ãƒªãƒƒãƒ‰æ•°
            
        Returns:
            dict: å®Œå…¨ãªåˆ†æçµæœ
        """
        self.logger.info(f"ğŸš€ Global Fire Monitoring and Anomaly Analysis v3.3 å®Œå…¨å®Ÿè¡Œé–‹å§‹")
        
        try:
            # 1. ãƒ‡ãƒ¼ã‚¿å‡¦ç† + ç•°å¸¸æ¤œçŸ¥
            results = self.process_yearly_data_with_anomaly_detection(year=year)
            
            # 2. ç•°å¸¸ã‚°ãƒªãƒƒãƒ‰ã®æ¨è«–åˆ†æ
            reasoning_results = self.analyze_anomalies_with_reasoning(top_n=top_anomalies)
            
            # 3. å¯è¦–åŒ–
            self.create_anomaly_visualization()
            
            # 4. çµæœã®çµ±åˆ
            complete_results = {
                'system_info': {
                    'name': 'Global Fire Monitoring and Anomaly Reasoning System',
                    'version': '3.3',
                    'processing_year': year,
                    'timestamp': datetime.now().isoformat()
                },
                'data_processing': results,
                'anomaly_reasoning': reasoning_results,
                'performance_metrics': {
                    'total_grids_processed': len(results['anomaly_detection']['anomaly_data']),
                    'anomalies_detected': results['anomaly_detection']['detection_summary']['anomalous_grids'],
                    'anomaly_rate': results['anomaly_detection']['detection_summary']['anomaly_rate'],
                    'reasoning_completed': len(reasoning_results.get('reasoning_results', []))
                }
            }
            
            # çµæœä¿å­˜
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"output/complete_anomaly_analysis_{timestamp}.json"
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(complete_results, f, indent=2, ensure_ascii=False, default=str)
            
            self.logger.info(f"âœ… å®Œå…¨åˆ†æçµæœä¿å­˜: {output_file}")
            self.logger.info("ğŸ‰ Global Fire Monitoring and Anomaly Analysis v3.3 å®Œäº†")
            
            return complete_results
            
        except Exception as e:
            self.logger.error(f"âŒ å®Œå…¨åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {'error': str(e)}


if __name__ == "__main__":
    # ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè¡Œä¾‹
    print("ğŸ”¥ Global Fire Monitoring and Anomaly Reasoning System v3.3")
    print("=" * 60)
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    system = GlobalFireMonitoringAndAnomalyReasoningSystemV33()
    
    # å®Œå…¨åˆ†æå®Ÿè¡Œ
    results = system.run_complete_anomaly_analysis(year=2022, top_anomalies=15)
    
    if 'error' not in results:
        print(f"\nâœ… åˆ†æå®Œäº†!")
        print(f"ğŸ“Š å‡¦ç†ã‚°ãƒªãƒƒãƒ‰æ•°: {results['performance_metrics']['total_grids_processed']:,}")
        print(f"ğŸš¨ ç•°å¸¸æ¤œçŸ¥æ•°: {results['performance_metrics']['anomalies_detected']:,}")
        print(f"ğŸ“ˆ ç•°å¸¸ç‡: {results['performance_metrics']['anomaly_rate']:.1%}")
        print(f"ğŸ¤– æ¨è«–å®Œäº†æ•°: {results['performance_metrics']['reasoning_completed']}")
    else:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {results['error']}")