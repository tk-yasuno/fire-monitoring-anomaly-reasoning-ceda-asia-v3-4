#!/usr/bin/env python3
"""
Isolation Forestç•°å¸¸æ¤œçŸ¥ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« for Global Fire Monitoring System v3.3
ã‚°ãƒªãƒƒãƒ‰å˜ä½ã§ã®ç«ç½ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œçŸ¥
"""

import numpy as np
import pandas as pd
import logging
from datetime import datetime
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import classification_report, silhouette_score
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

class FireAnomalyDetector:
    """
    Isolation Forest based ç«ç½ç•°å¸¸æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ 
    """
    
    def __init__(self, config=None):
        """
        åˆæœŸåŒ–
        
        Args:
            config (dict): ç•°å¸¸æ¤œçŸ¥è¨­å®š
        """
        self.logger = logging.getLogger(__name__)
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        self.config = config or {
            'contamination': 0.1,
            'n_estimators': 100,
            'random_state': 42,
            'bootstrap': False,
            'n_jobs': -1,
            'max_features': 1.0
        }
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.detector = None
        self.scaler = None
        self.feature_names = None
        self.is_fitted = False
        
        # çµæœä¿å­˜
        self.detection_results = {}
        self.feature_importance = {}
        
        self.logger.info("ğŸ” Fire Anomaly DetectoråˆæœŸåŒ–å®Œäº†")
    
    def prepare_features(self, data_df, feature_selection='auto'):
        """
        ç‰¹å¾´é‡æº–å‚™ã¨å‰å‡¦ç†
        
        Args:
            data_df (pd.DataFrame): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            feature_selection (str): ç‰¹å¾´é¸æŠæ–¹æ³•
            
        Returns:
            np.ndarray: å‰å‡¦ç†æ¸ˆã¿ç‰¹å¾´é‡
        """
        self.logger.info("ğŸ“Š ç‰¹å¾´é‡æº–å‚™é–‹å§‹")
        
        try:
            # æ•°å€¤åˆ—ã®è‡ªå‹•é¸æŠ
            if feature_selection == 'auto':
                numeric_columns = data_df.select_dtypes(include=[np.number]).columns.tolist()
                # é™¤å¤–ã™ã¹ãåˆ—
                exclude_columns = ['anomaly_prediction', 'anomaly_score', 'is_anomaly']
                numeric_columns = [col for col in numeric_columns if col not in exclude_columns]
            else:
                numeric_columns = feature_selection
            
            self.feature_names = numeric_columns
            
            # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            feature_data = data_df[numeric_columns].copy()
            
            # æ¬ æå€¤å‡¦ç†
            initial_shape = feature_data.shape
            feature_data = feature_data.fillna(feature_data.median())
            
            self.logger.info(f"ğŸ“Š ç‰¹å¾´é‡: {len(numeric_columns)}å€‹")
            self.logger.info(f"ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«: {len(feature_data)}å€‹")
            self.logger.info(f"ğŸ“Š æ¬ æå€¤å‡¦ç†: {initial_shape} â†’ {feature_data.shape}")
            
            # ç‰¹å¾´é‡çµ±è¨ˆ
            self._log_feature_statistics(feature_data)
            
            return feature_data.values
            
        except Exception as e:
            self.logger.error(f"âŒ ç‰¹å¾´é‡æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _log_feature_statistics(self, feature_data):
        """ç‰¹å¾´é‡çµ±è¨ˆã®ãƒ­ã‚°å‡ºåŠ›"""
        stats = feature_data.describe()
        
        self.logger.info("ğŸ“ˆ ç‰¹å¾´é‡çµ±è¨ˆ:")
        for col in feature_data.columns[:5]:  # æœ€åˆã®5ç‰¹å¾´é‡ã®ã¿è¡¨ç¤º
            mean_val = stats.loc['mean', col]
            std_val = stats.loc['std', col]
            self.logger.info(f"  {col}: å¹³å‡={mean_val:.3f}, æ¨™æº–åå·®={std_val:.3f}")
    
    def configure_detector(self, optimization=False, cv_folds=3):
        """
        Isolation Forestæ¤œçŸ¥å™¨ã®è¨­å®šã¨æœ€é©åŒ–
        
        Args:
            optimization (bool): ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’å®Ÿè¡Œã™ã‚‹ã‹
            cv_folds (int): äº¤å·®æ¤œè¨¼ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰æ•°
        """
        self.logger.info("âš™ï¸ Isolation Forestè¨­å®šé–‹å§‹")
        
        if optimization:
            self.logger.info("ğŸ”§ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–å®Ÿè¡Œä¸­...")
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰
            param_grid = {
                'contamination': [0.05, 0.1, 0.15, 0.2],
                'n_estimators': [50, 100, 200],
                'max_features': [0.5, 0.7, 1.0],
                'bootstrap': [False, True]
            }
            
            # åŸºæœ¬æ¤œçŸ¥å™¨
            base_detector = IsolationForest(
                random_state=self.config['random_state'],
                n_jobs=self.config['n_jobs']
            )
            
            # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒï¼ˆã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡é–¢æ•°ï¼‰
            grid_search = GridSearchCV(
                base_detector,
                param_grid,
                cv=cv_folds,
                scoring=self._custom_anomaly_scorer,
                n_jobs=1  # Isolation ForestãŒä¸¦åˆ—å‡¦ç†ã‚’è¡Œã†ãŸã‚
            )
            
            # æœ€é©åŒ–å®Ÿè¡Œï¼ˆãƒ€ãƒŸãƒ¼ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰
            # Isolation Forestã¯æ•™å¸«ãªã—å­¦ç¿’ã®ãŸã‚ã€ãƒ€ãƒŸãƒ¼ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’ä½¿ç”¨
            dummy_target = np.zeros(len(self.current_features))
            grid_search.fit(self.current_features, dummy_target)
            
            # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            best_params = grid_search.best_params_
            self.logger.info(f"âœ… æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {best_params}")
            
            # æœ€é©æ¤œçŸ¥å™¨ã®è¨­å®š
            self.detector = IsolationForest(**best_params, random_state=self.config['random_state'])
            
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
            self.detector = IsolationForest(
                contamination=self.config['contamination'],
                n_estimators=self.config['n_estimators'],
                random_state=self.config['random_state'],
                bootstrap=self.config['bootstrap'],
                n_jobs=self.config['n_jobs'],
                max_features=self.config['max_features']
            )
        
        self.logger.info("âœ… Isolation Forestè¨­å®šå®Œäº†")
    
    def _custom_anomaly_scorer(self, estimator, X, y=None):
        """ã‚«ã‚¹ã‚¿ãƒ ç•°å¸¸æ¤œçŸ¥è©•ä¾¡é–¢æ•°"""
        try:
            # ç•°å¸¸äºˆæ¸¬
            predictions = estimator.fit_predict(X)
            anomaly_scores = estimator.score_samples(X)
            
            # è©•ä¾¡æŒ‡æ¨™ã®çµ„ã¿åˆã‚ã›
            # 1. ç•°å¸¸ç‡ãŒç›®æ¨™ç¯„å›²å†…ã‹ã©ã†ã‹
            anomaly_rate = (predictions == -1).mean()
            rate_score = 1.0 - abs(anomaly_rate - 0.1) * 5  # 10%ã‚’ç›®æ¨™
            
            # 2. ç•°å¸¸ã‚¹ã‚³ã‚¢ã®åˆ†é›¢åº¦
            normal_scores = anomaly_scores[predictions == 1]
            anomaly_scores_subset = anomaly_scores[predictions == -1]
            
            if len(anomaly_scores_subset) > 0 and len(normal_scores) > 0:
                separation = abs(np.mean(normal_scores) - np.mean(anomaly_scores_subset))
            else:
                separation = 0
            
            # ç·åˆã‚¹ã‚³ã‚¢
            total_score = rate_score * 0.6 + min(1.0, separation) * 0.4
            
            return total_score
            
        except Exception:
            return 0.0
    
    def detect_anomalies(self, feature_data, scale_data=True):
        """
        ç•°å¸¸æ¤œçŸ¥å®Ÿè¡Œ
        
        Args:
            feature_data (np.ndarray): ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
            scale_data (bool): ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’è¡Œã†ã‹
            
        Returns:
            dict: æ¤œçŸ¥çµæœ
        """
        self.logger.info("ğŸ” ç•°å¸¸æ¤œçŸ¥é–‹å§‹")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            if scale_data:
                self.scaler = RobustScaler()  # å¤–ã‚Œå€¤ã«é ‘å¥
                scaled_features = self.scaler.fit_transform(feature_data)
                self.logger.info("ğŸ“ RobustScaleré©ç”¨")
            else:
                scaled_features = feature_data
            
            self.current_features = scaled_features
            
            # æ¤œçŸ¥å™¨è¨­å®š
            if self.detector is None:
                self.configure_detector()
            
            # ç•°å¸¸æ¤œçŸ¥å®Ÿè¡Œ
            start_time = datetime.now()
            
            predictions = self.detector.fit_predict(scaled_features)
            anomaly_scores = self.detector.score_samples(scaled_features)
            
            detection_time = (datetime.now() - start_time).total_seconds()
            
            # çµæœé›†ç´„
            results = self._compile_detection_results(
                predictions, anomaly_scores, feature_data, detection_time
            )
            
            self.detection_results = results
            self.is_fitted = True
            
            self.logger.info(f"âœ… ç•°å¸¸æ¤œçŸ¥å®Œäº†: {results['summary']['anomalous_count']}å€‹æ¤œå‡º")
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ç•°å¸¸æ¤œçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _compile_detection_results(self, predictions, anomaly_scores, original_features, detection_time):
        """æ¤œçŸ¥çµæœã®é›†ç´„"""
        
        # åŸºæœ¬çµ±è¨ˆ
        total_samples = len(predictions)
        anomalous_count = (predictions == -1).sum()
        anomaly_rate = anomalous_count / total_samples
        
        # ã‚¹ã‚³ã‚¢çµ±è¨ˆ
        score_stats = {
            'mean': np.mean(anomaly_scores),
            'std': np.std(anomaly_scores),
            'min': np.min(anomaly_scores),
            'max': np.max(anomaly_scores),
            'median': np.median(anomaly_scores)
        }
        
        # ç•°å¸¸ãƒ»æ­£å¸¸ã‚°ãƒ«ãƒ¼ãƒ—ã®åˆ†é›¢åº¦
        normal_scores = anomaly_scores[predictions == 1]
        anomaly_scores_subset = anomaly_scores[predictions == -1]
        
        separation_metrics = {}
        if len(anomaly_scores_subset) > 0 and len(normal_scores) > 0:
            separation_metrics = {
                'normal_mean': np.mean(normal_scores),
                'anomaly_mean': np.mean(anomaly_scores_subset),
                'separation_distance': abs(np.mean(normal_scores) - np.mean(anomaly_scores_subset)),
                'effect_size': abs(np.mean(normal_scores) - np.mean(anomaly_scores_subset)) / np.std(anomaly_scores)
            }
        
        # ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ
        feature_importance = self._analyze_feature_importance(original_features, predictions)
        
        results = {
            'summary': {
                'total_samples': total_samples,
                'anomalous_count': anomalous_count,
                'anomaly_rate': anomaly_rate,
                'detection_time': detection_time,
                'algorithm': 'isolation_forest'
            },
            'predictions': predictions,
            'anomaly_scores': anomaly_scores,
            'score_statistics': score_stats,
            'separation_metrics': separation_metrics,
            'feature_importance': feature_importance,
            'model_parameters': self.config,
            'timestamp': datetime.now().isoformat()
        }
        
        return results
    
    def _analyze_feature_importance(self, original_features, predictions):
        """ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ"""
        if self.feature_names is None:
            return {}
        
        importance_scores = {}
        
        # å„ç‰¹å¾´é‡ã«ã¤ã„ã¦ç•°å¸¸ãƒ»æ­£å¸¸ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®å·®ã‚’è¨ˆç®—
        for i, feature_name in enumerate(self.feature_names):
            feature_values = original_features[:, i]
            
            normal_values = feature_values[predictions == 1]
            anomaly_values = feature_values[predictions == -1]
            
            if len(anomaly_values) > 0 and len(normal_values) > 0:
                # å¹³å‡å€¤ã®å·®
                mean_diff = abs(np.mean(anomaly_values) - np.mean(normal_values))
                
                # æ¨™æº–åŒ–ã•ã‚ŒãŸå·®ï¼ˆCohen's dï¼‰
                pooled_std = np.sqrt(
                    ((len(normal_values) - 1) * np.var(normal_values) + 
                     (len(anomaly_values) - 1) * np.var(anomaly_values)) / 
                    (len(normal_values) + len(anomaly_values) - 2)
                )
                
                if pooled_std > 0:
                    cohens_d = mean_diff / pooled_std
                else:
                    cohens_d = 0
                
                # åˆ†å¸ƒã®é‡è¤‡åº¦
                overlap = self._calculate_distribution_overlap(normal_values, anomaly_values)
                
                # ç·åˆé‡è¦åº¦ï¼ˆ0-100ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
                importance = min(100, cohens_d * 20 * (1 - overlap))
                
                importance_scores[feature_name] = {
                    'importance_score': importance,
                    'mean_difference': mean_diff,
                    'cohens_d': cohens_d,
                    'distribution_overlap': overlap,
                    'normal_mean': np.mean(normal_values),
                    'anomaly_mean': np.mean(anomaly_values)
                }
        
        # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
        sorted_importance = dict(
            sorted(importance_scores.items(), 
                  key=lambda x: x[1]['importance_score'], 
                  reverse=True)
        )
        
        self.feature_importance = sorted_importance
        
        return sorted_importance
    
    def _calculate_distribution_overlap(self, values1, values2):
        """2ã¤ã®åˆ†å¸ƒã®é‡è¤‡åº¦è¨ˆç®—"""
        try:
            # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ãƒ™ãƒ¼ã‚¹ã®é‡è¤‡è¨ˆç®—
            min_val = min(np.min(values1), np.min(values2))
            max_val = max(np.max(values1), np.max(values2))
            
            bins = np.linspace(min_val, max_val, 50)
            
            hist1, _ = np.histogram(values1, bins=bins, density=True)
            hist2, _ = np.histogram(values2, bins=bins, density=True)
            
            # é‡è¤‡é¢ç©
            overlap = np.sum(np.minimum(hist1, hist2)) / len(bins)
            
            return overlap
            
        except Exception:
            return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    def generate_anomaly_report(self, data_df, output_path=None):
        """
        ç•°å¸¸æ¤œçŸ¥ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        
        Args:
            data_df (pd.DataFrame): å…ƒãƒ‡ãƒ¼ã‚¿
            output_path (str): å‡ºåŠ›ãƒ‘ã‚¹
            
        Returns:
            dict: ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿
        """
        if not self.is_fitted:
            self.logger.error("âŒ ç•°å¸¸æ¤œçŸ¥ãŒæœªå®Ÿè¡Œã§ã™")
            return {}
        
        self.logger.info("ğŸ“ ç•°å¸¸æ¤œçŸ¥ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹")
        
        try:
            # ç•°å¸¸ã‚°ãƒªãƒƒãƒ‰è©³ç´°åˆ†æ
            anomaly_details = self._analyze_anomaly_details(data_df)
            
            # åœ°ç†çš„åˆ†å¸ƒåˆ†æ
            geographic_analysis = self._analyze_geographic_distribution(data_df)
            
            # æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æï¼ˆæœˆåˆ¥ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆï¼‰
            temporal_analysis = self._analyze_temporal_patterns(data_df)
            
            # ãƒ¬ãƒãƒ¼ãƒˆçµ±åˆ
            report = {
                'detection_summary': self.detection_results['summary'],
                'anomaly_details': anomaly_details,
                'geographic_analysis': geographic_analysis,
                'temporal_analysis': temporal_analysis,
                'feature_importance': self.feature_importance,
                'model_performance': self._evaluate_model_performance(),
                'recommendations': self._generate_recommendations(),
                'metadata': {
                    'generation_time': datetime.now().isoformat(),
                    'total_features': len(self.feature_names) if self.feature_names else 0,
                    'model_parameters': self.config
                }
            }
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
            if output_path:
                import json
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(report, f, indent=2, ensure_ascii=False, default=str)
                self.logger.info(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_path}")
            
            return report
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def _analyze_anomaly_details(self, data_df):
        """ç•°å¸¸ã‚°ãƒªãƒƒãƒ‰è©³ç´°åˆ†æ"""
        predictions = self.detection_results['predictions']
        anomaly_scores = self.detection_results['anomaly_scores']
        
        # ç•°å¸¸ã‚°ãƒªãƒƒãƒ‰ã®æŠ½å‡º
        anomaly_mask = predictions == -1
        anomaly_data = data_df[anomaly_mask].copy()
        anomaly_data['anomaly_score'] = anomaly_scores[anomaly_mask]
        
        if len(anomaly_data) == 0:
            return {'message': 'ç•°å¸¸ã‚°ãƒªãƒƒãƒ‰ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ'}
        
        # ç•°å¸¸å¼·åº¦ã«ã‚ˆã‚‹åˆ†é¡
        score_threshold_critical = np.percentile(anomaly_scores[anomaly_mask], 10)  # ä¸‹ä½10%
        score_threshold_high = np.percentile(anomaly_scores[anomaly_mask], 30)     # ä¸‹ä½30%
        
        critical_anomalies = anomaly_data[anomaly_data['anomaly_score'] <= score_threshold_critical]
        high_anomalies = anomaly_data[
            (anomaly_data['anomaly_score'] > score_threshold_critical) & 
            (anomaly_data['anomaly_score'] <= score_threshold_high)
        ]
        moderate_anomalies = anomaly_data[anomaly_data['anomaly_score'] > score_threshold_high]
        
        details = {
            'total_anomalies': len(anomaly_data),
            'severity_distribution': {
                'critical': len(critical_anomalies),
                'high': len(high_anomalies),
                'moderate': len(moderate_anomalies)
            },
            'score_statistics': {
                'min_score': float(np.min(anomaly_scores[anomaly_mask])),
                'max_score': float(np.max(anomaly_scores[anomaly_mask])),
                'mean_score': float(np.mean(anomaly_scores[anomaly_mask])),
                'std_score': float(np.std(anomaly_scores[anomaly_mask]))
            },
            'top_anomalies': self._get_top_anomalies(anomaly_data, top_n=10)
        }
        
        return details
    
    def _get_top_anomalies(self, anomaly_data, top_n=10):
        """ä¸Šä½ç•°å¸¸ã‚°ãƒªãƒƒãƒ‰ã®å–å¾—"""
        top_anomalies = anomaly_data.nsmallest(top_n, 'anomaly_score')
        
        results = []
        for idx, row in top_anomalies.iterrows():
            anomaly_info = {
                'grid_id': idx,
                'anomaly_score': float(row['anomaly_score']),
                'location': {
                    'latitude': float(row.get('latitude', 0)),
                    'longitude': float(row.get('longitude', 0)),
                    'continent': row.get('continent', 'Unknown')
                },
                'key_features': {}
            }
            
            # é‡è¦ç‰¹å¾´é‡ã®å€¤
            if self.feature_importance:
                top_features = list(self.feature_importance.keys())[:5]
                for feature in top_features:
                    if feature in row:
                        anomaly_info['key_features'][feature] = float(row[feature])
            
            results.append(anomaly_info)
        
        return results
    
    def _analyze_geographic_distribution(self, data_df):
        """åœ°ç†çš„åˆ†å¸ƒåˆ†æ"""
        predictions = self.detection_results['predictions']
        
        try:
            # å¤§é™¸åˆ¥åˆ†æ
            if 'continent' in data_df.columns:
                continent_analysis = data_df.groupby('continent').agg({
                    'latitude': 'count',
                }).rename(columns={'latitude': 'total_grids'})
                
                # ç•°å¸¸ã‚°ãƒªãƒƒãƒ‰æ•°
                anomaly_mask = predictions == -1
                anomaly_by_continent = data_df[anomaly_mask].groupby('continent').size()
                continent_analysis['anomaly_grids'] = anomaly_by_continent.fillna(0)
                continent_analysis['anomaly_rate'] = continent_analysis['anomaly_grids'] / continent_analysis['total_grids']
                
                geographic_analysis = {
                    'continent_distribution': continent_analysis.to_dict('index'),
                    'highest_risk_continent': continent_analysis['anomaly_rate'].idxmax(),
                    'lowest_risk_continent': continent_analysis['anomaly_rate'].idxmin()
                }
            else:
                geographic_analysis = {'message': 'å¤§é™¸ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“'}
            
            return geographic_analysis
            
        except Exception as e:
            return {'error': str(e)}
    
    def _analyze_temporal_patterns(self, data_df):
        """æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        try:
            predictions = self.detection_results['predictions']
            anomaly_mask = predictions == -1
            
            # æœˆåˆ¥ç«ç½ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®åˆ†æ
            month_columns = [col for col in data_df.columns if col.startswith('month_') and col.endswith('_fire')]
            
            if month_columns:
                # æœˆåˆ¥ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³
                anomaly_data = data_df[anomaly_mask]
                normal_data = data_df[~anomaly_mask]
                
                monthly_patterns = {}
                for month_col in month_columns:
                    month_num = int(month_col.split('_')[1])
                    
                    anomaly_mean = anomaly_data[month_col].mean() if len(anomaly_data) > 0 else 0
                    normal_mean = normal_data[month_col].mean() if len(normal_data) > 0 else 0
                    
                    monthly_patterns[month_num] = {
                        'anomaly_mean': float(anomaly_mean),
                        'normal_mean': float(normal_mean),
                        'difference': float(anomaly_mean - normal_mean)
                    }
                
                # æœ€ã‚‚ç•°å¸¸ãŒé¡•è‘—ãªæœˆ
                max_diff_month = max(monthly_patterns.items(), key=lambda x: abs(x[1]['difference']))
                
                temporal_analysis = {
                    'monthly_patterns': monthly_patterns,
                    'peak_anomaly_month': max_diff_month[0],
                    'seasonal_effect': 'detected' if len(monthly_patterns) > 6 else 'insufficient_data'
                }
            else:
                temporal_analysis = {'message': 'æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“'}
            
            return temporal_analysis
            
        except Exception as e:
            return {'error': str(e)}
    
    def _evaluate_model_performance(self):
        """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½è©•ä¾¡"""
        if not self.detection_results:
            return {}
        
        try:
            # åˆ†é›¢å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹
            separation_metrics = self.detection_results.get('separation_metrics', {})
            
            # æ¤œçŸ¥ç‡è©•ä¾¡
            target_anomaly_rate = self.config['contamination']
            actual_anomaly_rate = self.detection_results['summary']['anomaly_rate']
            rate_accuracy = 1 - abs(target_anomaly_rate - actual_anomaly_rate) / target_anomaly_rate
            
            # ç·åˆå“è³ªã‚¹ã‚³ã‚¢
            quality_components = {
                'rate_accuracy': rate_accuracy,
                'separation_quality': min(1.0, separation_metrics.get('effect_size', 0) / 2),
                'feature_utilization': len(self.feature_importance) / len(self.feature_names) if self.feature_names else 0
            }
            
            overall_quality = np.mean(list(quality_components.values()))
            
            performance = {
                'quality_components': quality_components,
                'overall_quality_score': float(overall_quality),
                'separation_metrics': separation_metrics,
                'detection_efficiency': {
                    'target_anomaly_rate': target_anomaly_rate,
                    'actual_anomaly_rate': float(actual_anomaly_rate),
                    'rate_accuracy': float(rate_accuracy)
                }
            }
            
            return performance
            
        except Exception as e:
            return {'error': str(e)}
    
    def _generate_recommendations(self):
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = {
            'model_optimization': [],
            'monitoring_enhancement': [],
            'data_quality': []
        }
        
        # ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–æ¨å¥¨
        if self.detection_results:
            anomaly_rate = self.detection_results['summary']['anomaly_rate']
            target_rate = self.config['contamination']
            
            if abs(anomaly_rate - target_rate) / target_rate > 0.3:
                recommendations['model_optimization'].append(
                    f"ç•°å¸¸æ¤œçŸ¥ç‡èª¿æ•´ãŒå¿…è¦ (ç¾åœ¨: {anomaly_rate:.1%}, ç›®æ¨™: {target_rate:.1%})"
                )
            
            if len(self.feature_importance) < 5:
                recommendations['model_optimization'].append(
                    "ç‰¹å¾´é‡ã®è¿½åŠ ã¾ãŸã¯ç‰¹å¾´å·¥å­¦ã®æ”¹å–„ã‚’æ¤œè¨"
                )
        
        # ç›£è¦–å¼·åŒ–æ¨å¥¨
        recommendations['monitoring_enhancement'].extend([
            "ç•°å¸¸ã‚¹ã‚³ã‚¢é–¾å€¤ã®å‹•çš„èª¿æ•´ã‚·ã‚¹ãƒ†ãƒ å°å…¥",
            "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç•°å¸¸ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½ã®å®Ÿè£…",
            "ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è‡ªå‹•åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰"
        ])
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªæ¨å¥¨
        recommendations['data_quality'].extend([
            "ç‰¹å¾´é‡ã®ç›¸é–¢åˆ†æã¨å†—é•·æ€§é™¤å»",
            "å¤–ã‚Œå€¤ã®äº‹å‰å‡¦ç†æ‰‹æ³•æ”¹å–„",
            "ãƒ‡ãƒ¼ã‚¿åé›†é »åº¦ã®æœ€é©åŒ–"
        ])
        
        return recommendations
    
    def visualize_anomaly_detection(self, data_df, save_path=None):
        """ç•°å¸¸æ¤œçŸ¥çµæœã®å¯è¦–åŒ–"""
        if not self.is_fitted:
            self.logger.error("âŒ ç•°å¸¸æ¤œçŸ¥ãŒæœªå®Ÿè¡Œã§ã™")
            return
        
        self.logger.info("ğŸ“Š ç•°å¸¸æ¤œçŸ¥å¯è¦–åŒ–é–‹å§‹")
        
        try:
            # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
            plt.rcParams['font.family'] = ['Yu Gothic', 'Hiragino Kaku Gothic Pro', 'sans-serif']
            plt.rcParams['axes.unicode_minus'] = False
            
            predictions = self.detection_results['predictions']
            anomaly_scores = self.detection_results['anomaly_scores']
            
            # 4ãƒ‘ãƒãƒ«å¯è¦–åŒ–
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('Isolation Forest Fire Anomaly Detection Results', fontsize=16, fontweight='bold')
            
            # 1. ç•°å¸¸ã‚¹ã‚³ã‚¢åˆ†å¸ƒ
            axes[0, 0].hist(anomaly_scores, bins=50, alpha=0.7, color='lightblue', edgecolor='black')
            axes[0, 0].axvline(np.percentile(anomaly_scores, 10), color='red', linestyle='--', 
                              label=f'Anomaly Threshold (10%)')
            axes[0, 0].set_xlabel('Anomaly Score')
            axes[0, 0].set_ylabel('Frequency')
            axes[0, 0].set_title('Anomaly Score Distribution')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
            
            # 2. åœ°ç†çš„åˆ†å¸ƒ
            if 'latitude' in data_df.columns and 'longitude' in data_df.columns:
                normal_mask = predictions == 1
                anomaly_mask = predictions == -1
                
                axes[0, 1].scatter(data_df.loc[normal_mask, 'longitude'], 
                                  data_df.loc[normal_mask, 'latitude'],
                                  c='lightblue', alpha=0.5, s=10, label='Normal')
                axes[0, 1].scatter(data_df.loc[anomaly_mask, 'longitude'], 
                                  data_df.loc[anomaly_mask, 'latitude'],
                                  c='red', alpha=0.8, s=30, label='Anomaly')
                axes[0, 1].set_xlabel('Longitude')
                axes[0, 1].set_ylabel('Latitude')
                axes[0, 1].set_title('Geographic Distribution')
                axes[0, 1].legend()
                axes[0, 1].grid(True, alpha=0.3)
            
            # 3. ç‰¹å¾´é‡é‡è¦åº¦
            if self.feature_importance:
                top_features = list(self.feature_importance.keys())[:8]
                importance_scores = [self.feature_importance[f]['importance_score'] for f in top_features]
                
                axes[1, 0].barh(range(len(top_features)), importance_scores, color='green', alpha=0.7)
                axes[1, 0].set_yticks(range(len(top_features)))
                axes[1, 0].set_yticklabels([f.replace('_', ' ').title() for f in top_features])
                axes[1, 0].set_xlabel('Importance Score')
                axes[1, 0].set_title('Feature Importance for Anomaly Detection')
                axes[1, 0].grid(True, alpha=0.3)
            
            # 4. å¤§é™¸åˆ¥ç•°å¸¸ç‡
            if 'continent' in data_df.columns:
                continent_stats = data_df.groupby('continent').agg({'latitude': 'count'}).rename(columns={'latitude': 'total'})
                anomaly_by_continent = data_df[predictions == -1].groupby('continent').size()
                continent_stats['anomalies'] = anomaly_by_continent.fillna(0)
                continent_stats['anomaly_rate'] = continent_stats['anomalies'] / continent_stats['total']
                
                axes[1, 1].bar(range(len(continent_stats)), continent_stats['anomaly_rate'], 
                              color='orange', alpha=0.7)
                axes[1, 1].set_xticks(range(len(continent_stats)))
                axes[1, 1].set_xticklabels(continent_stats.index, rotation=45, ha='right')
                axes[1, 1].set_ylabel('Anomaly Rate')
                axes[1, 1].set_title('Anomaly Rate by Continent')
                axes[1, 1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # ä¿å­˜
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                self.logger.info(f"âœ… å¯è¦–åŒ–ä¿å­˜: {save_path}")
            
            plt.show()
            
        except Exception as e:
            self.logger.error(f"âŒ å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    print("ğŸ” Fire Anomaly Detector ãƒ†ã‚¹ãƒˆ")
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)
    n_samples = 1000
    
    sample_data = pd.DataFrame({
        'latitude': np.random.uniform(-60, 70, n_samples),
        'longitude': np.random.uniform(-180, 180, n_samples),
        'continent': np.random.choice(['Africa', 'Asia', 'North America', 'Europe'], n_samples),
        'fire_activity': np.random.gamma(2, 2, n_samples),
        'burned_area_total': np.random.exponential(5, n_samples),
        'neighbor_max': np.random.gamma(3, 3, n_samples),
        'neighbor_std': np.random.gamma(2, 1, n_samples),
        'temperature_avg': np.random.normal(25, 10, n_samples),
        'precipitation_total': np.random.exponential(50, n_samples)
    })
    
    # ç•°å¸¸æ¤œçŸ¥å™¨åˆæœŸåŒ–
    detector = FireAnomalyDetector()
    
    # ç‰¹å¾´é‡æº–å‚™
    features = detector.prepare_features(sample_data)
    
    # ç•°å¸¸æ¤œçŸ¥å®Ÿè¡Œ
    results = detector.detect_anomalies(features)
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    sample_data['anomaly_prediction'] = results['predictions']
    sample_data['anomaly_score'] = results['anomaly_scores']
    
    report = detector.generate_anomaly_report(sample_data)
    
    print(f"âœ… ç•°å¸¸æ¤œçŸ¥å®Œäº†: {results['summary']['anomalous_count']}å€‹æ¤œå‡º")
    print(f"ğŸ“Š ç•°å¸¸ç‡: {results['summary']['anomaly_rate']:.1%}")